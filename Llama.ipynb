{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b608a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./build/\")\n",
    "\n",
    "import fastLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c787e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 convert-pth-to-ggml.py ~/Models/LLaMa/65B/ 1 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd63bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_model_load: loading model from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 512\n",
      "llama_model_load: n_embd  = 8192\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 64\n",
      "llama_model_load: n_layer = 80\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 1\n",
      "llama_model_load: n_ff    = 22016\n",
      "llama_model_load: n_parts = 8\n",
      "llama_model_load: ggml ctx size = 127085.23 MB\n",
      "llama_model_load: memory_size =  2560.00 MB, n_mem = 40960\n",
      "llama_model_load: loading model part 1/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 2/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.1'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 3/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.2'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 4/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.3'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 5/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.4'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 6/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.5'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 7/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.6'\n",
      "llama_model_load: .......................................................................................... done\n",
      "llama_model_load: model size = 15570.03 MB / num tensors = 723\n",
      "llama_model_load: loading model part 8/8 from '/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin.7'\n",
      "llama_model_load: .........................................................................................."
     ]
    }
   ],
   "source": [
    "#Initializing the Model\n",
    "MODEL_PATH = \"/Users/jdonaldson/Models/LLaMA/65B/ggml-model-f16.bin\"\n",
    "MODEL_ID = \"LLAMA-65B\"\n",
    "model = fastLlama.Model(\n",
    "        id=MODEL_ID,\n",
    "        path=MODEL_PATH, #path to model\n",
    "        num_threads=8, #number of threads to use\n",
    "        n_ctx=512, #context size of model\n",
    "        last_n_size=64, #size of last n tokens (used for repetition penalty) (Optional)\n",
    "        seed=0 #seed for random number generator (Optional)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c757bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n",
    "\n",
    "User: Hello, Bob.\n",
    "Bob: Hello. How may I help you today?\n",
    "User: Please tell me the largest city in Europe.\n",
    "Bob: Sure. The largest city in Europe is Moscow, the capital of Russia.\n",
    "User: \"\"\"\n",
    "\n",
    "res = model.ingest(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2bf397e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa251a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bob, how tall is the Eiffel Tower?"
     ]
    }
   ],
   "source": [
    "def stream_token(x: str) -> None:\n",
    "    \"\"\"\n",
    "    This function is called by the library to stream tokens\n",
    "    \"\"\"\n",
    "    print(x, end='', flush=True)\n",
    "\n",
    "res = model.generate(\n",
    "    num_tokens=100, \n",
    "    top_p=0.95, #top p sampling (Optional)\n",
    "    temp=0.8, #temperature (Optional)\n",
    "    repeat_penalty=1.0, #repetition penalty (Optional)\n",
    "    streaming_fn=stream_token, #streaming function\n",
    "    stop_word=[\"User:\", \"\\n\"] #stop generation when this word is encountered (Optional)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83684289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
